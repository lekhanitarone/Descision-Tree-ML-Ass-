{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **Question 1:**\n",
        "\n",
        "**What is a Decision Tree, and how does it work in the context of classification?**\n",
        "\n",
        "**Answer:**\n",
        ">A Decision Tree is a supervised learning algorithm used for classification and regression tasks.\n",
        "It works by splitting the dataset into smaller parts based on feature values to form a tree-like model.\n",
        "Each internal node represents a decision rule on a feature, and each leaf node gives a class label.\n",
        "The algorithm selects the feature that best separates the data using impurity measures like Gini or Entropy.\n",
        "This process continues recursively until no further improvement can be made.\n",
        "Decision Trees aim to create the purest possible subsets for accurate prediction.\n",
        "They are easy to interpret because they follow simple “if-else” decision paths.\n",
        "Hence, they’re widely used for transparent and rule-based classification tasks.\n",
        "\n",
        "---\n",
        "\n",
        "### **Question 2:**\n",
        "\n",
        "**Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?**\n",
        "\n",
        "**Answer:**\n",
        ">Gini Impurity and Entropy are measures used to determine the quality of splits in a Decision Tree.\n",
        "Gini Impurity calculates how often a randomly chosen element would be misclassified.\n",
        "Entropy measures the level of uncertainty or disorder in a node.\n",
        "Both values are low when the node is pure (contains mostly one class).\n",
        "The algorithm tries to split data to reduce these impurity values as much as possible.\n",
        "This reduction is called Information Gain when using entropy.\n",
        "Lower impurity means better-separated and more accurate nodes.\n",
        "Thus, these measures directly guide how the Decision Tree grows and makes predictions.\n",
        "\n",
        "\n",
        "### **Question 3:**\n",
        "\n",
        "**What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.**\n",
        "\n",
        "**Answer:**\n",
        ">Pre-Pruning (also called *Early Stopping*) stops the tree from growing once a certain condition is met, such as a minimum number of samples or maximum depth.\n",
        "It prevents the model from becoming too complex and overfitting early in the training stage.\n",
        "Post-Pruning, on the other hand, allows the tree to grow fully and then removes branches that provide little or no improvement in accuracy.\n",
        "This helps simplify the model while keeping its predictive power strong.\n",
        "An advantage of Pre-Pruning is that it saves training time and computational resources.\n",
        "An advantage of Post-Pruning is that it produces a more generalized model with better real-world performance.\n",
        "Both methods help control overfitting but differ in when they are applied.\n",
        "Thus, pruning ensures the Decision Tree remains accurate yet interpretable.\n",
        "\n",
        "---\n",
        "\n",
        "### **Question 4:**\n",
        "\n",
        "**What is Information Gain in Decision Trees, and why is it important for choosing the best split?**\n",
        "\n",
        "**Answer:**\n",
        ">Information Gain (IG) measures how much uncertainty or impurity is reduced after splitting the dataset on a particular feature.\n",
        "It is calculated as the difference between the parent node’s entropy and the weighted average entropy of its child nodes.\n",
        "A higher Information Gain indicates a more effective split that better separates the classes.\n",
        "The Decision Tree algorithm selects the feature with the highest IG at each step for splitting.\n",
        "This helps ensure that each node becomes purer and more meaningful for classification.\n",
        "By maximizing Information Gain, the tree improves prediction accuracy and efficiency.\n",
        "It plays a key role in building an optimal tree structure with clear decision boundaries.\n",
        "Hence, IG guides the learning process toward the most informative features.\n",
        "\n",
        "\n",
        "\n",
        "### **Question 5:**\n",
        "\n",
        "**What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?**\n",
        "\n",
        "**Answer:**\n",
        ">Decision Trees are widely used in finance for credit risk assessment and loan approval decisions.\n",
        "In healthcare, they help diagnose diseases and predict patient outcomes.\n",
        "In marketing, they’re used for customer segmentation and predicting buying behavior.\n",
        "Their main advantage is interpretability — results can be easily understood through “if-else” rules.\n",
        "They can also handle both numerical and categorical data effectively.\n",
        "However, Decision Trees often overfit the training data if not properly pruned.\n",
        "They can also be unstable, as small data changes may lead to a different structure.\n",
        "Despite these limits, they remain popular for their simplicity, clarity, and strong baseline performance.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fAXxwCr60i4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info:\n",
        "# ● Iris Dataset for classification tasks (sklearn.datasets.load_iris() or\n",
        "# provided CSV).\n",
        "# ● Boston Housing Dataset for regression tasks\n",
        "# (sklearn.datasets.load_boston() or provided CSV).\n",
        "\n",
        "# Question 6: Write a Python program to:\n",
        "# ● Load the Iris Dataset\n",
        "# ● Train a Decision Tree Classifier using the Gini criterion\n",
        "# ● Print the model’s accuracy and feature importances\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Decision Tree using Gini criterion\n",
        "model = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate model\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Feature Importances:\", model.feature_importances_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3N8gvOUo3yOx",
        "outputId": "b2917827-8f89-4190-f9a1-23f2496c7ac9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "Feature Importances: [0.         0.01911002 0.89326355 0.08762643]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 7: Write a Python program to:\n",
        "# ● Load the Iris Dataset\n",
        "# ● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "# a fully-grown tree.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train limited-depth and full tree models\n",
        "tree_limited = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "tree_full = DecisionTreeClassifier(random_state=42)\n",
        "tree_limited.fit(X_train, y_train)\n",
        "tree_full.fit(X_train, y_train)\n",
        "\n",
        "# Compare accuracies\n",
        "print(\"Accuracy (max_depth=3):\", accuracy_score(y_test, tree_limited.predict(X_test)))\n",
        "print(\"Accuracy (full tree):\", accuracy_score(y_test, tree_full.predict(X_test)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p64XsAlM4Yki",
        "outputId": "fc9e449a-da53-4608-c885-dae90d183485"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (max_depth=3): 1.0\n",
            "Accuracy (full tree): 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 8: Write a Python program to:\n",
        "# ● Load the California Housing dataset from sklearn\n",
        "# ● Train a Decision Tree Regressor\n",
        "# ● Print the Mean Squared Error (MSE) and feature importances\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California Housing dataset\n",
        "california = fetch_california_housing()\n",
        "X = california.data       # Features\n",
        "y = california.target     # Target (median house value)\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Create Decision Tree Regressor\n",
        "dt_regressor = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "# Train the model\n",
        "dt_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = dt_regressor.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for feature_name, importance in zip(california.feature_names, dt_regressor.feature_importances_):\n",
        "    print(f\"{feature_name}: {importance:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrU_kP4s4nZp",
        "outputId": "925ca8e7-2859-46d3-bbad-472b34d825a8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 0.53\n",
            "Feature Importances:\n",
            "MedInc: 0.5235\n",
            "HouseAge: 0.0521\n",
            "AveRooms: 0.0494\n",
            "AveBedrms: 0.0250\n",
            "Population: 0.0322\n",
            "AveOccup: 0.1390\n",
            "Latitude: 0.0900\n",
            "Longitude: 0.0888\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 9: Write a Python program to:\n",
        "# ● Load the Iris Dataset\n",
        "# ● Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "# GridSearchCV\n",
        "# ● Print the best parameters and the resulting model accuracy\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Create Decision Tree classifier\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Define the hyperparameter grid to search\n",
        "param_grid = {\n",
        "    'max_depth': [None, 2, 3, 4, 5],\n",
        "    'min_samples_split': [2, 3, 4, 5]\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=dt, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit GridSearchCV to training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters\n",
        "best_params = grid_search.best_params_\n",
        "print(f\"Best Parameters: {best_params}\")\n",
        "\n",
        "# Predict with the best model\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy with Best Parameters: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VzCTBzVn5H3l",
        "outputId": "bde840d3-f779-4bae-bdf4-0428dbc44634"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': None, 'min_samples_split': 2}\n",
            "Accuracy with Best Parameters: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10**\n",
        "\n",
        "Imagine you’re working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "\n",
        "● Handle the missing values\n",
        "● Encode the categorical features\n",
        "● Train a Decision Tree model\n",
        "● Tune its hyperparameters\n",
        "● Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "\n",
        "### **1️. Handle Missing Values**\n",
        "\n",
        "* Analyze missing data.\n",
        "* Impute numerical features (mean/median) and categorical features (mode or ‘Unknown’).\n",
        "* Drop features with too many missing values if necessary.\n",
        "\n",
        "\n",
        "\n",
        "### **2. Encode Categorical Features**\n",
        "\n",
        "* Use Label Encoding for ordinal data.\n",
        "* Use One-Hot Encoding for nominal data.\n",
        "\n",
        "\n",
        "\n",
        "### **3. Train Decision Tree Model**\n",
        "\n",
        "* Split data into training and testing sets.\n",
        "* Initialize and fit a Decision Tree classifier.\n",
        "\n",
        "\n",
        "\n",
        "### **4️. Tune Hyperparameters**\n",
        "\n",
        "* Key hyperparameters: `max_depth`, `min_samples_split`, `min_samples_leaf`, `criterion`.\n",
        "* Use GridSearchCV or RandomizedSearchCV to find the best combination.\n",
        "\n",
        "\n",
        "\n",
        "### **5️. Evaluate Performance**\n",
        "\n",
        "* Use metrics beyond accuracy: Precision, Recall, F1-Score, ROC-AUC.\n",
        "* Confusion matrix helps understand false positives/negatives, crucial in healthcare.\n",
        "\n",
        "\n",
        "\n",
        "### **6️. Business Value**\n",
        "\n",
        "* Early disease detection and risk stratification.\n",
        "* Supports clinical decisions and prioritizes resources.\n",
        "* Reduces unnecessary tests and costs.\n",
        "* Enables population health management and preventive care.\n",
        "\n"
      ],
      "metadata": {
        "id": "DOsYUsjE6K2H"
      }
    }
  ]
}